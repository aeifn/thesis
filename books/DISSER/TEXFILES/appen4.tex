\section*{Приложение 4. К главе 3: Доказательства 
теорем~\ref{sim} и~\ref{princip}}

{\bf Доказательство теоремы~\ref{sim}}

Наша игра~--- это игра с континуумом агентов, пронумерованных точками отрезка
$[0,1]$ и имеющих стратегическое множество $Z=[0,1]$, и функции выигрыша
\begin{equation}
\label{game7}
u(\theta) = u(\th,z,q) =
z \cdot [b(\theta) - D \cdot \lambda_q(z)],
\end{equation}
где $\lambda_q(z)$ задаётся формулой
\begin{equation}
\label{intens}
\lambda_q(z) =: \lambda_{\ad}(z,q) = \sum_{\{i: z_i < z\}}
\frac{A_i}{1-F(z_i)}.
\end{equation}
Надо проверить, что для нашей игры выполнены свойства, постулированные в
определениях 1,2 и 3. Начнём с первого из них.

Для $q^2 \succeq q^1$ мы имеем следующее уравнение:
\begin{equation}
\label{nabor3}
\delta(z) = u^2(z) - u^1(z) =
z \cdot (\lambda_{q^1} (z) - \lambda_{q^2} (z)),
\end{equation}
и достаточно показать, что выражение внутри скобок неотрицательное и
неубывает по $z$. Из~(\ref{intens}) следует, что выражение в скобках в
правой части формулы~(\ref{nabor3}) является $i$-той частичной суммой 
некоторого ряда, и с ростом $z$ имеем, что $i$ также неубывает. Если мы 
убедимся в том, что члены ряда неотрицательны, то свойство 1 будет 
доказано, ибо тогда частичные суммы неубывают и неотрицательны 
(и помножаются на $z$, которое тоже положительное и 
возрастающее~--- тавтология). Имеем:
\begin{equation}
\label{chain3}
\begin{array}{c}
q^2 \succeq q^1 \, \Longrightarrow \, \forall z \quad 
F_{q_1}(z) \ge F_{q_2}(z) \Longrightarrow
\\
1-F_{q_1}(z) \le 1-F_{q_2}(z) \, \Longrightarrow
\frac{A_j}{1-F_{q_1}(z_j)} - \frac{A_j}{1-F_{q_2}(z_j)} > 0,
\end{array}
\end{equation}
но это как раз и есть члены ряда. Таким образом, свойство 1 установлено.

Свойство 2 очевидно, ибо для $\th' > \th$ выражение
\begin{equation}
\label{coi4}
\delta(z) = u^2(z) - u^1(z) = z \cdot (b(\th') - b(\th))
\end{equation}
положительно и возрастает по $z$ (или просто равно $0$).

Третье свойство (постулированное в определении 3) следует из того факта, что
выигрыш агента~(\ref{game7}) зависит от $\lambda_q(z)$, которое в свою очередь
зависит от значений $F_q(z')$ для $z'<z$, в силу~(\ref{intens}). Последние,
естественно, зависят только от $q|_{[0,z)}$ для профилей $q \in \Re$.
Теорема~\ref{sim} полностью доказана.

\subsection*{Доказательство теоремы~\ref{princip}}

Мы будем существенно использовать теорему~\ref{itera3} из Приложения 1.
Рассмотрим стратегию $\ad$, вместе с соответствующим ей СНР 
$q^* = (z^*_1,\dots,z^*_n)$. Итеративная процедура
\begin{equation}
\label{quii}
\begin{array}{c}
z^*_1 = h(I) (\th_1),
\\
\dots
\\
z^*_j = h(z^*_1,\dots,z^*_{j-1}, I) (\th_j),
\\
\dots
\\
z^*_n = h(z^*_1,\dots,z^*_{n-1},1) (\th_n)
\end{array}
\end{equation}
приводит к равновесию не более, чем за $n$ шагов. Сперва мы построим
$(n+1)$-ступенчатую стратегию, имплементирующую то же самое равновесие $q^*$.
А именно, это будет стратегия $(B_0, z^*_0; \, B_1,z^*_1; \dots; B_n,z^*_n)$,
где $z^*_0 = 0$, и для $\forall j=0,1,\dots,n$ мы имеем
\begin{equation}
\label{instead}
B_j = \sum_{\{l \, | \, z_l \in [z_j, z_{j+1})\}} A_l.
\end{equation}

Покажем, что в процессе эволюции профиля $I$ согласно 
формулам~(\ref{quji}) из Приложения 1 ни один агент не поменяет свой выбор.
Будем рассуждать индуктивно. Обозначим за $u_i(z)$ результирующую функцию
выигрыша агентов с типом $i$ на той стадии, когда они осуществляют свой выбор
согласно итерационному процессу~(\ref{quji}), и за $\tilde u_i(z)$~---
ту же функцию выигрыша, соответствующую новой стратегии~(\ref{instead})
контролирующего органа.

Начнём с агентов первого типа (рассуждения несколько различается при $i=1$ и
$i>1$). Их функция выигрыша, соответствующая вере в профиль $I$, теперь
описывается следующим образом:
\begin{equation}
\label{chan}
\begin{array}{c}
\forall z < z^*_1 \quad \tilde u_1 (z) = b_1 \cdot z;
\\
\forall z \ge z^*_1 \quad \tilde u_1 (z) \le u_1 (z)
\end{array}
\end{equation}
(чтобы с этим согласиться, достаточно взглянуть внимательно на рисунок 2а).
Из первой строки следует, что новый выбор таких агентов не ниже старого 
выбора, $z^*_1$. В то же время, так как $z^*_1$ был их выбором, 
соответствующим стратегии $\ad$, мы имеем
\begin{equation}
\label{pia}
\forall z > z^*_1 \quad
\tilde u_1 (z^*_1) = b_1 \cdot z^*_1 \ge u_1 (z^*_1)
\ge u_1 (z) \ge \tilde u_1 (z),
\end{equation}
откуда становится ясно, что выбор агентов первого типа не поменялся (первое
неравенство в~(\ref{pia}) следует из того, что выигрыш агентов первого типа,
выбирающих $z=z^*_1$, не превосходит чистого взяточного сбора
$b_1 \cdot z^*_1$).

Теперь воспользуемся индукцией, и предположим, что агенты с типами
$\th_j, j=1,2,\dots,i$ не поменяли своего выбора. Тогда агент с типом $i+1$, 
осуществляя свой выбор в соответствии с формулой~(\ref{quii}), исходит из
того же профиля, как и раньше. Но тогда его выбор поменяться не может, ибо,
с одной стороны, мы знаем, что
\begin{equation}
\label{fact3}
\tilde u_{i+1} (z^*_{i+1}) = u_{i+1} (z^*_{i+1})
\end{equation}
(это следует из $I$-свойства), а с другой, во всех остальных точках
интенсивность аудита, $\lambda$ не может уменьшится, следовательно,
\begin{equation}
\label{thelast}
\forall z \quad \tilde u_i (z) \le u_i (z).
\end{equation}
При этом мы существенно воспользовались тем, что агент исходил из того же 
профиля, что и раньше, по предположению индукции. Поэтому выигрыш не 
изменился в точке арг-максимума, и не возрос в остальных точках. 
Следовательно, агент не изменит своего выбора.

Всё, что осталось заметить,~--- это то, что, во-первых, новая стратегия
использует не больше ресурсов, чем старая, и во-вторых, что ресурсы,
предназначенные для поголовных проверок, могут быть безболезненно
перемещены в точку первого ненулевого порогового значения (что уменьшает
число необходимых параметров стратегии с $2n+2$ до $2n$).

Теорема~\ref{princip} доказана.